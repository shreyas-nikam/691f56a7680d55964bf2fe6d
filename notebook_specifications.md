
# Technical Specification for Jupyter Notebook: Interactive Clustering Workbench

## 1. Notebook Overview

### Learning Goals
This Jupyter Notebook serves as an interactive workbench for exploring and understanding various unsupervised clustering techniques. Upon completion, users will be able to:
- Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms as detailed in [2], [4], and [6] respectively.
- Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods, affinity kernels) on clustering results.
- Visualize cluster assignments in 2D/3D space and interpret dendrograms generated by hierarchical clustering.
- Gain insights into how clustering can be applied for tasks such as financial market segmentation or portfolio construction, with specific demonstrations using synthetic financial data for k-Means stock portfolio construction [3] and Spectral Clustering for identifying correlated assets [5].

## 2. Code Requirements

### List of Expected Libraries
The following Python libraries are expected to be imported and utilized:
- `numpy` for numerical operations.
- `pandas` for data manipulation and loading.
- `scikit-learn` (`sklearn.cluster`, `sklearn.preprocessing`, `sklearn.datasets`, `sklearn.metrics`) for clustering algorithms, data scaling, dataset generation, and evaluation metrics.
- `matplotlib.pyplot` for 2D static visualizations.
- `seaborn` for enhanced statistical data visualization.
- `plotly.express` for interactive 2D/3D visualizations.
- `scipy.cluster.hierarchy` for hierarchical clustering specific utilities like dendrogram generation.
- `ipywidgets` for creating interactive controls within the notebook.

### List of Algorithms or Functions to be Implemented
The notebook will demonstrate and utilize the following algorithms and functions (without their internal code implementation):
- **Data Generation:**
    - `sklearn.datasets.make_blobs` to generate synthetic datasets with distinct or overlapping clusters.
- **Data Preprocessing:**
    - `sklearn.preprocessing.StandardScaler` for feature scaling.
    - `sklearn.decomposition.PCA` for dimensionality reduction to 2D/3D for visualization.
- **Clustering Algorithms:**
    - `sklearn.cluster.KMeans` (implementation of Figure 1).
    - `sklearn.cluster.SpectralClustering` (implementation of Figure 2).
    - `sklearn.cluster.AgglomerativeClustering` (implementation of Figure 3).
- **Cluster Evaluation:**
    - `sklearn.metrics.silhouette_score` for internal cluster validation.
- **Visualization Functions:**
    - Custom Python functions to generate 2D and 3D scatter plots of data points colored by cluster assignments.
    - Custom Python function to generate an interactive dendrogram using `scipy.cluster.hierarchy.dendrogram`.
- **Interactive Controls:**
    - `ipywidgets.IntSlider` for numerical parameter tuning (e.g., number of clusters $k$).
    - `ipywidgets.Dropdown` for categorical parameter selection (e.g., linkage method, affinity kernel).
    - `ipywidgets.interact` to link widgets with visualization and clustering functions for dynamic updates.

### Visualization Requirements
The notebook will generate the following types of visualizations:
- **2D Scatter Plots:** Display data points colored by their assigned clusters, including cluster centroids where applicable. Plots must include clear axis labels, titles, and legends.
- **3D Scatter Plots (Optional/As needed for higher dimensions):** For datasets with 3 primary components after PCA, display data points in 3D space, distinctly colored by their assigned clusters, with interactive rotation capabilities (e.g., using `plotly.express`).
- **Interactive Dendrogram:** For Hierarchical Clustering, an interactive dendrogram similar to Exhibit 2, visualizing the merging process of clusters. This will be generated using `scipy.cluster.hierarchy`.
- **Output Tables:** Tables summarizing clustering metrics (e.g., Silhouette Score for different parameter settings).

## 3. Notebook Sections (in detail)

### 1. Introduction to Unsupervised Clustering
- **Markdown Cell:**
  This notebook explores various unsupervised clustering techniques. Unsupervised learning discovers hidden patterns in data without predefined labels, making it invaluable for tasks like market segmentation and portfolio construction in finance. We will focus on k-Means, Spectral Clustering, and Agglomerative Hierarchical Clustering.

  **Learning Outcomes:**
  - Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms.
  - Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods) on clustering results.
  - Visualize cluster assignments in 2D/3D space and interpret dendrograms.
  - Gain insights into how clustering can be applied for tasks like market segmentation or portfolio construction.

### 2. Setting Up the Environment and Data Utilities
- **Markdown Cell:**
  We begin by importing necessary libraries for data handling, clustering, visualization, and interactive controls. We will also define utility functions for loading our pre-loaded financial datasets and for preprocessing them.
- **Code Cell (Imports):**
  Imports: `numpy as np`, `pandas as pd`, `sklearn.cluster`, `sklearn.preprocessing`, `sklearn.datasets`, `sklearn.metrics`, `matplotlib.pyplot as plt`, `seaborn as sns`, `plotly.express as px`, `scipy.cluster.hierarchy as sch`, `ipywidgets as widgets`, `IPython.display as display`.
- **Code Cell (Utility Functions):**
  Define a function `load_synthetic_financial_data(dataset_type)` that generates and returns a synthetic dataset based on the `dataset_type` string (e.g., 'kmeans_portfolio', 'spectral_assets').
  For `kmeans_portfolio`: `X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)` scaled to represent financial features.
  For `spectral_assets`: `X, y_true = make_moons(n_samples=300, noise=0.05, random_state=42)` for non-linear structures, followed by `PCA(n_components=2)` if needed for 2D. (Let's stick to `make_blobs` with more overlap for financial relevance if `make_moons` feels too artificial for asset returns). Let's use `make_blobs` for both, but adjust `cluster_std` for `spectral_assets` to simulate more intertwined relationships, and transform with `PCA` if we want 2D projections. For simplicity, just `make_blobs` directly for 2D/3D.
  Define `preprocess_data(data)`: applies `StandardScaler` from `sklearn.preprocessing` to the input data.
- **Markdown Cell:**
  The `load_synthetic_financial_data` function generates specific synthetic datasets that mimic financial market data characteristics for demonstration purposes. The `preprocess_data` function ensures that all features are scaled to a standard range, which is crucial for distance-based clustering algorithms.

### 3. Pre-loaded Financial Market Data for K-Means Demo
- **Markdown Cell:**
  For the k-Means demonstration, we'll use a synthetic financial dataset representing "stock features for portfolio construction". This dataset will consist of features like simulated growth rates, volatility, and dividend yields for different stocks. We will first load and scale this data.
- **Code Cell (Load and Preprocess):**
  `kmeans_data = load_synthetic_financial_data('kmeans_portfolio')`
  `scaled_kmeans_data = preprocess_data(kmeans_data)`
  `print("Shape of pre-loaded K-Means data:", scaled_kmeans_data.shape)`
- **Markdown Cell:**
  The `scaled_kmeans_data` is now ready for clustering. This dataset consists of several data points (representing stocks) each with standardized features, suitable for identifying distinct groups for portfolio construction.

### 4. K-Means Clustering: Theory
- **Markdown Cell:**
  k-Means clustering is a partition-based algorithm that aims to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid). The algorithm iteratively assigns data points to clusters and updates cluster centroids until convergence.

  The objective function, often called the inertia, that k-Means aims to minimize is the sum of squared distances of samples to their closest cluster center:
  $$ J = \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2) $$
  where $J$ is the objective function, $x_i$ is a data point, $\mu_j$ is the centroid of cluster $j$, and $C$ is the set of all centroids. The algorithm typically proceeds as described in Figure 1 [2]:
  1. Initialize $k$ centroids randomly.
  2. For a fixed number of iterations:
     a. Assign each data point to its closest centroid.
     b. Update each centroid to be the mean of all points assigned to that cluster.
     c. If centroids do not change significantly, break.
  3. Output cluster assignments and final centroids.
- **Code Cell (Function Definition):**
  `def run_kmeans(data, n_clusters, random_state=42):`
  `    """`
  `    Runs K-Means clustering on the provided data.`
  `    Args: data (np.ndarray), n_clusters (int), random_state (int)`
  `    Returns: cluster_labels (np.ndarray), cluster_centers (np.ndarray)`
  `    """`
  `    # Utilize sklearn.cluster.KMeans`
  `    # Fit the model and return labels and centers`
- **Markdown Cell:**
  The `run_kmeans` function encapsulates the k-Means algorithm, allowing us to easily apply it with different parameters.

### 5. K-Means Clustering: Execution and Visualization
- **Markdown Cell:**
  We will now execute k-Means clustering on our synthetic financial data with an initial choice of $k=3$ clusters, as commonly explored in portfolio diversification. We will then visualize the resulting clusters.
- **Code Cell (Execution):**
  `kmeans_labels, kmeans_centers = run_kmeans(scaled_kmeans_data, n_clusters=3)`
  `print("First 5 cluster assignments:", kmeans_labels[:5])`
- **Code Cell (Visualization):**
  `# Prepare data for Plotly visualization (e.g., converting to DataFrame if needed)`
  `# Add cluster labels to the DataFrame`
  `# Plot 2D scatter plot using plotly.express.scatter`
  `# x-axis: 'Feature 1', y-axis: 'Feature 2'`
  `# color by 'Cluster'`
  `# title: 'K-Means Clustering (k=3) of Stock Features'`
  `# Add centroids to the plot`
  `# (Optional) If data has >2 features, use PCA for 2D visualization first:`
  `# pca = PCA(n_components=2)`
  `# data_2d = pca.fit_transform(scaled_kmeans_data)`
  `# Then plot data_2d`
- **Markdown Cell:**
  The scatter plot visually represents the 3 distinct clusters identified by the k-Means algorithm. Each color corresponds to a different cluster, and the black 'x' markers indicate the calculated centroids. In a financial context, these clusters could represent different types of stocks (e.g., growth stocks, value stocks, defensive stocks), aiding in diversified portfolio construction.

### 6. K-Means Clustering: Interactive Parameter Tuning and Evaluation
- **Markdown Cell:**
  The choice of the number of clusters, $k$, is critical for k-Means. We can evaluate the quality of clustering using metrics like the Silhouette Score. A higher Silhouette Score generally indicates better-defined clusters. The Silhouette Coefficient $s(i)$ for a single sample is calculated as:
  $$ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} $$
  where $a(i)$ is the mean distance between $i$ and all other data points in the same cluster, and $b(i)$ is the mean distance between $i$ and all other data points in the *next nearest* cluster.

  We can interactively adjust $k$ and observe its impact on the clusters and the Silhouette Score.
- **Code Cell (Interactive Tuning):**
  `@widgets.interact(n_clusters=widgets.IntSlider(min=2, max=10, step=1, value=3, description='Number of Clusters (k):'))`
  `def interactive_kmeans(n_clusters):`
  `    # Run k-Means with selected n_clusters`
  `    # Calculate silhouette_score using sklearn.metrics.silhouette_score(scaled_kmeans_data, kmeans_labels)`
  `    # Print Silhouette Score`
  `    # Generate and display 2D scatter plot with new cluster assignments and centroids`
  `    # Title: f'K-Means Clustering with k={n_clusters}'`
- **Markdown Cell:**
  By adjusting the slider, you can observe how the cluster boundaries and assignments change. The Silhouette Score provides a quantitative measure of clustering quality, helping to identify a suitable number of clusters for the given dataset. Typically, we look for a $k$ that yields a high silhouette score while making sense contextually.

### 7. Pre-loaded Financial Market Data for Spectral Clustering Demo
- **Markdown Cell:**
  For Spectral Clustering, we'll utilize another synthetic financial dataset representing "asset returns for correlated asset identification". This dataset will simulate returns of various assets that might exhibit non-linear correlations or lie on manifolds, making it suitable for Spectral Clustering to identify groups of correlated assets.
- **Code Cell (Load and Preprocess):**
  `spectral_data = load_synthetic_financial_data('spectral_assets')`
  `scaled_spectral_data = preprocess_data(spectral_data)`
  `print("Shape of pre-loaded Spectral Clustering data:", scaled_spectral_data.shape)`
- **Markdown Cell:**
  The `scaled_spectral_data` is now ready. This dataset represents assets whose relationships might be better captured by graph-based similarity rather than direct Euclidean distance, which is where Spectral Clustering excels.

### 8. Spectral Clustering: Theory
- **Markdown Cell:**
  Spectral Clustering is a technique that uses the eigenvalues (spectrum) of a similarity matrix to perform dimensionality reduction before clustering in a lower-dimensional space. It is particularly effective for discovering non-globular or intertwined clusters. The process generally follows Figure 2 [4]:
  1.  **Construct similarity matrix $W$**: Measures the similarity between all pairs of data points. A common choice is the Gaussian kernel:
      $$ W[i, j] = \exp\left(-\frac{||x_i - x_j||^2}{2\sigma^2}\right) $$
      where $x_i$ and $x_j$ are data points, and $\sigma$ is a scaling parameter.
  2.  **Compute degree matrix $D$**: A diagonal matrix where $D[i, i]$ is the sum of similarities of data point $i$ with all other data points:
      $$ D[i, i] = \sum_j W[i, j] $$
  3.  **Compute normalized Laplacian $L_{norm}$**: A matrix often used to reveal the graph structure:
      $$ L_{norm} = D^{-1/2} \times (D - W) \times D^{-1/2} $$
  4.  **Find $k$ smallest eigenvectors of $L_{norm}$**: These eigenvectors form a new lower-dimensional representation of the data.
  5.  **Form matrix $V$**: Consisting of these $k$ eigenvectors as columns.
  6.  **Normalize rows of $V$ to unit length**.
  7.  **Apply k-Means clustering to the rows of $V$**: This groups the data points in the transformed space.
  8.  **Output cluster assignments**.
- **Code Cell (Function Definition):**
  `def run_spectral_clustering(data, n_clusters, affinity_kernel='rbf', gamma=1.0, random_state=42):`
  `    """`
  `    Runs Spectral Clustering on the provided data.`
  `    Args: data (np.ndarray), n_clusters (int), affinity_kernel (str), gamma (float), random_state (int)`
  `    Returns: cluster_labels (np.ndarray)`
  `    """`
  `    # Utilize sklearn.cluster.SpectralClustering`
  `    # Fit the model and return labels`
- **Markdown Cell:**
  The `run_spectral_clustering` function simplifies the application of Spectral Clustering with flexible parameters like the number of clusters and the affinity kernel.

### 9. Spectral Clustering: Execution and Visualization
- **Markdown Cell:**
  We will now apply Spectral Clustering to our asset returns data with $k=3$ clusters, using an 'rbf' (Radial Basis Function) affinity kernel.
- **Code Cell (Execution):**
  `spectral_labels = run_spectral_clustering(scaled_spectral_data, n_clusters=3, affinity_kernel='rbf', gamma=1.0)`
  `print("First 5 cluster assignments:", spectral_labels[:5])`
- **Code Cell (Visualization):**
  `# Prepare data for Plotly visualization`
  `# Add cluster labels to the DataFrame`
  `# Plot 2D scatter plot using plotly.express.scatter`
  `# x-axis: 'Feature 1', y-axis: 'Feature 2'`
  `# color by 'Cluster'`
  `# title: 'Spectral Clustering (k=3) of Asset Returns'`
  `# (Optional) If data has >2 features, use PCA for 2D visualization first.`
- **Markdown Cell:**
  The visualization shows the clusters identified by Spectral Clustering. This method can uncover groupings that are not linearly separable, which is particularly useful in financial contexts for identifying complex relationships between assets, such as groups of assets that exhibit similar behavior under certain market regimes.

### 10. Spectral Clustering: Interactive Parameter Tuning and Evaluation
- **Markdown Cell:**
  Similar to k-Means, the number of clusters $k$ is an important parameter. Additionally, the `affinity` kernel (how similarity is defined) significantly impacts Spectral Clustering. We can explore these parameters interactively, along with the Silhouette Score.
- **Code Cell (Interactive Tuning):**
  `@widgets.interact(n_clusters=widgets.IntSlider(min=2, max=10, step=1, value=3, description='Number of Clusters (k):'),`
  `                 affinity_kernel=widgets.Dropdown(options=['rbf', 'nearest_neighbors'], value='rbf', description='Affinity Kernel:'))`
  `def interactive_spectral(n_clusters, affinity_kernel):`
  `    # Run Spectral Clustering with selected parameters`
  `    # Calculate silhouette_score`
  `    # Print Silhouette Score`
  `    # Generate and display 2D scatter plot`
  `    # Title: f'Spectral Clustering with k={n_clusters}, affinity={affinity_kernel}'`
- **Markdown Cell:**
  Observe how changes in `n_clusters` and the `affinity_kernel` affect the resulting clusters and the Silhouette Score. Different affinity kernels might be more appropriate for different underlying data structures. For example, 'rbf' is good for capturing dense regions, while 'nearest_neighbors' focuses on local connectivity.

### 11. Agglomerative Hierarchical Clustering: Theory
- **Markdown Cell:**
  Agglomerative Hierarchical Clustering builds a hierarchy of clusters from individual data points. It is a "bottom-up" approach, where each data point starts as its own cluster, and then pairs of clusters are iteratively merged based on their proximity until all points belong to a single cluster or a desired number of clusters is reached. This process is detailed in Figure 3 [6].
  
  Key concepts include:
  -   **Distance Metric**: How the distance between individual data points is measured (e.g., Euclidean, Manhattan).
  -   **Linkage Method**: How the distance between two clusters is defined. Common methods (described on page 5) include:
      -   **Single Linkage**: The shortest distance between any two points in the two clusters.
          $$ d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y) $$
      -   **Complete Linkage**: The maximum distance between any two points in the two clusters.
          $$ d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y) $$
      -   **Average Linkage**: The average distance between all pairs of points across the two clusters.
          $$ d(C_i, C_j) = \text{mean}_{x \in C_i, y \in C_j} d(x, y) $$
      -   **Ward Linkage**: Minimizes the variance of the clusters being merged. It calculates the increase in the total within-cluster variance after merging.
- **Code Cell (Function Definition):**
  `def run_hierarchical_clustering(data, n_clusters, linkage_method, affinity_metric):`
  `    """`
  `    Runs Agglomerative Hierarchical Clustering.`
  `    Args: data (np.ndarray), n_clusters (int), linkage_method (str), affinity_metric (str)`
  `    Returns: cluster_labels (np.ndarray)`
  `    """`
  `    # Utilize sklearn.cluster.AgglomerativeClustering`
  `    # Fit the model and return labels`

  `def plot_dendrogram(data, linkage_matrix, n_clusters_display=None, title="Hierarchical Clustering Dendrogram"):`
  `    """`
  `    Generates and displays a dendrogram.`
  `    Args: data (np.ndarray), linkage_matrix (np.ndarray), n_clusters_display (int), title (str)`
  `    """`
  `    # Utilize scipy.cluster.hierarchy.dendrogram`
  `    # Set figure size, plot the dendrogram, add title`
  `    # (Optional) Add a horizontal line to indicate cluster cut-off for n_clusters_display`
- **Markdown Cell:**
  The `run_hierarchical_clustering` function allows us to experiment with different linkage methods and distance metrics. The `plot_dendrogram` function provides a visual representation of the hierarchical structure, as shown in Exhibit 2, which is crucial for understanding the merging process.

### 12. Agglomerative Hierarchical Clustering: Execution and Visualization
- **Markdown Cell:**
  We will now apply Agglomerative Hierarchical Clustering to our synthetic financial data, using the 'ward' linkage method and 'euclidean' distance, aiming for 3 clusters. We will visualize both the resulting clusters in a scatter plot and the dendrogram.
- **Code Cell (Execution):**
  `agg_labels = run_hierarchical_clustering(scaled_kmeans_data, n_clusters=3, linkage_method='ward', affinity_metric='euclidean')`
  `print("First 5 cluster assignments:", agg_labels[:5])`
- **Code Cell (Visualization - Scatter Plot):**
  `# Prepare data for Plotly visualization`
  `# Add cluster labels to the DataFrame`
  `# Plot 2D scatter plot using plotly.express.scatter`
  `# x-axis: 'Feature 1', y-axis: 'Feature 2'`
  `# color by 'Cluster'`
  `# title: 'Agglomerative Hierarchical Clustering (k=3)'`
  `# (Optional) If data has >2 features, use PCA for 2D visualization first.`
- **Code Cell (Visualization - Dendrogram):**
  `# Compute the linkage matrix for dendrogram generation`
  `# Z = sch.linkage(scaled_kmeans_data, method='ward', metric='euclidean')`
  `# plot_dendrogram(scaled_kmeans_data, Z, n_clusters_display=3)`
- **Markdown Cell:**
  The scatter plot shows the clusters identified by hierarchical clustering. The dendrogram provides a tree-like diagram that illustrates the sequence of merges or splits. By cutting the dendrogram at a specific height, one can determine the cluster assignments, allowing for visual inspection of cluster formation at various levels of granularity.

### 13. Agglomerative Hierarchical Clustering: Interactive Parameter Tuning and Evaluation
- **Markdown Cell:**
  The power of hierarchical clustering lies in exploring different linkage methods and distance metrics. We can interactively adjust these parameters along with the desired number of clusters ($k$) to understand their impact on the clustering structure and the Silhouette Score.
- **Code Cell (Interactive Tuning):**
  `@widgets.interact(n_clusters=widgets.IntSlider(min=2, max=10, step=1, value=3, description='Number of Clusters (k):'),`
  `                 linkage_method=widgets.Dropdown(options=['ward', 'complete', 'average', 'single'], value='ward', description='Linkage Method:'),`
  `                 affinity_metric=widgets.Dropdown(options=['euclidean', 'l1', 'l2', 'manhattan', 'cosine'], value='euclidean', description='Distance Metric:'))`
  `def interactive_hierarchical(n_clusters, linkage_method, affinity_metric):`
  `    # Run Hierarchical Clustering with selected parameters`
  `    # Calculate silhouette_score`
  `    # Print Silhouette Score`
  `    # Generate and display 2D scatter plot`
  `    # Title: f'Agglomerative Hierarchical Clustering (k={n_clusters}, linkage={linkage_method})'`
  `    # (Optional) Re-compute and display dendrogram if performance allows`
- **Markdown Cell:**
  Experiment with different linkage methods and distance metrics. You will observe how these choices significantly influence how clusters are formed and how sensitive the algorithm is to different data distributions. For instance, 'single' linkage can find elongated clusters, while 'ward' tends to find more compact, spherical clusters. The Silhouette Score helps quantify the effectiveness of these parameter choices.

### 14. User Dataset Upload and Analysis
- **Markdown Cell:**
  This workbench also allows you to upload your own datasets for clustering analysis. Your dataset should be in a CSV format and contain numerical features. The notebook will automatically preprocess (scale) your data before applying the clustering algorithms.
  
  **Instructions:**
  1.  Ensure your CSV file contains only numerical features relevant for clustering.
  2.  Run the code cell below to trigger the upload mechanism.
  3.  Once uploaded and processed, you can re-run the clustering cells (e.g., K-Means, Spectral, Hierarchical) with your new dataset by replacing `scaled_kmeans_data` or `scaled_spectral_data` with `scaled_user_data`.
- **Code Cell (Upload and Preprocess):**
  `# Placeholder for user file upload mechanism`
  `# Example for typical Jupyter environment (this will require manual file selection in a browser)`
  `# import ipywidgets as widgets`
  `# from IPython.display import display`
  `# upload_button = widgets.FileUpload(accept='.csv', multiple=False)`
  `# display(upload_button)`
  `# user_data = pd.read_csv(io.BytesIO(list(upload_button.value.values())[0]['content']))`
  `# For simpler demonstration without actual file upload widget for direct execution,`
  `# simulate loading from a known local path or provide instructions.`
  `# Example: For a direct spec, assume user provides path or a widget is used.`
  `# For this specification, we assume a mechanism exists and data is loaded into `user_data_df`.`
  `# For concrete implementation, you'd replace this with actual upload code.`
  `# Example: user_data_df = pd.read_csv('path/to/your/uploaded_data.csv')`
  `# For this specification, let's use a dummy dataset to show the flow:`
  `user_data_df = pd.DataFrame(np.random.rand(100, 4), columns=[f'User_Feature_{i}' for i in range(4)])`
  `scaled_user_data = preprocess_data(user_data_df.values)`
  `print("Shape of uploaded and scaled user data:", scaled_user_data.shape)`
  `print("First 5 rows of scaled user data:")`
  `print(scaled_user_data[:5])`
- **Markdown Cell:**
  Your custom dataset has been successfully loaded and scaled. You can now proceed to apply any of the clustering algorithms (k-Means, Spectral, Hierarchical) to `scaled_user_data` and interactively explore the clustering results.

