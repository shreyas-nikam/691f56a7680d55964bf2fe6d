id: 691f56a7680d55964bf2fe6d_user_guide
summary: Unsupervised learning User Guide
feedback link: https://docs.google.com/forms/d/e/1FAIpQLSfWkOK-in_bMMoHSZfcIvAeO58PAH9wrDqcxnJABHaxiDqhSA/viewform?usp=sf_link
environments: Web
status: Published
# QuLab: Interactive Exploration of Unsupervised Clustering in Finance

## 1. Introduction to Unsupervised Clustering and QuLab
Duration: 00:05
In this lab, we delve into the fascinating world of **Unsupervised Learning**, specifically focusing on clustering techniques. Unsupervised learning is a paradigm in machine learning where algorithms learn patterns from unlabeled data, meaning there are no pre-defined output variables. This is particularly useful in finance for tasks such as identifying distinct market segments, grouping similar assets for portfolio diversification, or detecting anomalous trading behavior.

We will explore three fundamental clustering algorithms:
1.  **k-Means Clustering**: A partition-based algorithm that aims to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid).
2.  **Spectral Clustering**: A technique that uses the eigenvalues (spectrum) of a similarity matrix to perform dimensionality reduction before clustering in a lower-dimensional space, often effective for non-globular clusters.
3.  **Agglomerative Hierarchical Clustering**: A "bottom-up" approach that builds a hierarchy of clusters by progressively merging similar clusters, visualized through a dendrogram.

Through interactive demonstrations and parameter tuning, you will gain hands-on experience in applying these algorithms, understanding their underlying principles, and interpreting their results in a financial context. We will also examine the impact of various parameters, such as the number of clusters ($k$), linkage methods, and affinity kernels, on the final clustering outcomes.

### Learning Goals
Upon completion of this application, users will be able to:
- Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms.
- Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods, affinity kernels) on clustering results.
- Visualize cluster assignments in 2D/3D space and interpret dendrograms generated by hierarchical clustering.
- Gain insights into how clustering can be applied for tasks such as financial market segmentation or portfolio construction, with specific demonstrations using synthetic financial data.

## 2. Navigating QuLab and Preparing Data
Duration: 00:05
The QuLab application features a sidebar on the left that allows you to control the data source and navigate between the different clustering algorithms.

### Data Source Selection
At the top of the sidebar, you'll find the **"Data Source Selection"** section:
- **K-Means Portfolio Data (Demo)**: This option loads a synthetic dataset ideal for demonstrating k-Means clustering, typically exhibiting distinct, globular clusters.
- **Spectral Assets Data (Demo)**: This option loads a synthetic dataset designed to showcase Spectral Clustering's ability to identify non-globular or intertwined clusters.
- **Upload Your Own Data**: This allows you to upload a CSV file from your local machine.

<aside class="positive">
<b>Tip:</b> Always ensure you have a data source selected before proceeding with clustering. If you upload your own data, the application automatically preprocesses it using a StandardScaler to ensure all features are scaled to a standard range, which is crucial for distance-based clustering algorithms.
</aside>

### Navigation Between Clustering Algorithms
Below the data source selection, there's a **"Navigation"** dropdown. Use this to switch between the three main clustering techniques:
- **K-Means Clustering**
- **Spectral Clustering**
- **Agglomerative Hierarchical Clustering**

For each clustering method, the main area of the application will update to display the relevant theoretical explanations, interactive controls, and visualizations.

## 3. K-Means Clustering: Theory and Interaction
Duration: 00:10
Navigate to the **"K-Means Clustering"** option using the sidebar dropdown.

### Understanding the K-Means Algorithm
k-Means clustering is a partition-based algorithm that aims to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid). The algorithm iteratively assigns data points to clusters and updates cluster centroids until convergence.

The objective function, often called the inertia, that k-Means aims to minimize is the sum of squared distances of samples to their closest cluster center:
$$ J = \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2) $$
where $J$ is the objective function, $x_i$ is a data point, $\mu_j$ is the centroid of cluster $j$, and $C$ is the set of all centroids. The algorithm typically proceeds as follows:
1. Initialize $k$ centroids randomly.
2. For a fixed number of iterations:
   a. Assign each data point to its closest centroid.
   b. Update each centroid to be the mean of all points assigned to that cluster.
   c. If centroids do not change significantly, break.
3. Output cluster assignments and final centroids.

### Interactive Parameter Tuning and Evaluation
The choice of the number of clusters, $k$, is critical for k-Means. We can evaluate the quality of clustering using metrics like the Silhouette Score. A higher Silhouette Score generally indicates better-defined clusters, where data points are well-matched to their own cluster and poorly matched to neighboring clusters. The Silhouette Coefficient $s(i)$ for a single sample is calculated as:
$$ s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} $$
where $a(i)$ is the mean distance between $i$ and all other data points in the same cluster, and $b(i)$ is the mean distance between $i$ and all other data points in the *next nearest* cluster.

You can interactively adjust $k$ using the **'Number of Clusters (k) for K-Means:'** slider and observe its impact on the clusters and the Silhouette Score. This helps in finding an optimal $k$ that balances clustering quality with business interpretability.

The scatter plot visually represents the distinct clusters identified by the k-Means algorithm. Each color corresponds to a different cluster, and the black 'x' markers indicate the calculated centroids. In a financial context, these clusters could represent different types of stocks (e.g., growth stocks, value stocks, defensive stocks), aiding in diversified portfolio construction by selecting assets from different segments.
By adjusting the slider, you can observe how the cluster boundaries and assignments change. The Silhouette Score provides a quantitative measure of clustering quality, helping to identify a suitable number of clusters for the given dataset.

## 4. Spectral Clustering: Theory and Interaction
Duration: 00:15
Now, switch to the **"Spectral Clustering"** option using the sidebar dropdown.

### Understanding the Spectral Clustering Algorithm
Spectral Clustering is a technique that uses the eigenvalues (spectrum) of a similarity matrix to perform dimensionality reduction before clustering in a lower-dimensional space. It is particularly effective for discovering non-globular or intertwined clusters by leveraging the graph structure of the data. The process generally follows these steps:
1.  **Construct similarity matrix $W$**: Measures the similarity between all pairs of data points. A common choice is the Gaussian (Radial Basis Function) kernel:
    $$ W[i, j] = \exp\left(-\frac{||x_i - x_j||^2}{2\sigma^2}\right) $$
    where $x_i$ and $x_j$ are data points, and $\sigma$ is a scaling parameter that controls the width of the similarity decay.
2.  **Compute degree matrix $D$**: A diagonal matrix where $D[i, i]$ is the sum of similarities of data point $i$ with all other data points:
    $$ D[i, i] = \sum_j W[i, j] $$
3.  **Compute normalized Laplacian $L_{norm}$**: A matrix often used to reveal the graph structure and its connected components:
    $$ L_{norm} = D^{-1/2} \times (D - W) \times D^{-1/2} $$
4.  **Find $k$ smallest eigenvectors of $L_{norm}$**: These eigenvectors correspond to the lowest frequencies in the graph and form a new lower-dimensional, spectrally transformed representation of the data.
5.  **Form matrix $V$**: Consisting of these $k$ eigenvectors as columns.
6.  **Normalize rows of $V$ to unit length**.
7.  **Apply k-Means clustering to the rows of $V$**: This groups the data points in the transformed, lower-dimensional space.
8.  **Output cluster assignments**.

### Interactive Parameter Tuning and Evaluation
Similar to k-Means, the number of clusters $k$ is an important parameter for Spectral Clustering. Additionally, the `affinity` kernel (how similarity is defined) significantly impacts the clustering results. You can explore these parameters interactively, along with the Silhouette Score, to understand their influence on cluster formation and quality.

Use the **'Number of Clusters (k) for Spectral Clustering:'** slider and the **'Affinity Kernel for Spectral Clustering:'** selectbox to observe how changes in these parameters affect the resulting clusters and the Silhouette Score. Different affinity kernels might be more appropriate for different underlying data structures. For example, 'rbf' is good for capturing dense regions and non-linear boundaries, while 'nearest_neighbors' focuses on local connectivity. This interactive exploration helps in selecting parameters that yield meaningful and stable clusters for financial assets.

## 5. Agglomerative Hierarchical Clustering: Theory and Interaction
Duration: 00:20
Finally, navigate to the **"Agglomerative Hierarchical Clustering"** option using the sidebar dropdown.

### Understanding the Agglomerative Hierarchical Clustering Algorithm
Agglomerative Hierarchical Clustering builds a hierarchy of clusters from individual data points. It is a "bottom-up" approach, where each data point starts as its own cluster, and then pairs of clusters are iteratively merged based on their proximity until all points belong to a single cluster or a desired number of clusters is reached. This process forms a tree-like structure called a dendrogram.

Key concepts include:
-   **Distance Metric**: How the distance between individual data points is measured (e.g., Euclidean, Manhattan, Cosine). This defines the proximity between any two data points.
-   **Linkage Method**: How the distance between two clusters is defined based on the distances between their constituent data points. Common methods include:
    -   **Single Linkage**: The shortest distance between any two points in the two clusters. This method is prone to chaining, where clusters are merged due to single close points.
        $$ d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y) $$
    -   **Complete Linkage**: The maximum distance between any two points in the two clusters. This tends to produce compact, spherical clusters.
        $$ d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y) $$
    -   **Average Linkage**: The average distance between all pairs of points across the two clusters. This offers a compromise between single and complete linkage.
        $$ d(C_i, C_j) = \text{mean}_{x \in C_i, y \in C_j} d(x, y) $$
    -   **Ward Linkage**: Minimizes the variance of the clusters being merged. It calculates the increase in the total within-cluster variance after merging, generally favoring compact, spherical clusters of similar size.

Understanding these methods is crucial for financial applications, as the choice impacts how asset groups are formed (e.g., tightly correlated groups vs. loosely related ones).

### Interactive Parameter Tuning and Evaluation
The power of hierarchical clustering lies in exploring different linkage methods and distance metrics. You can interactively adjust these parameters along with the desired number of clusters ($k$) to understand their impact on the clustering structure and the Silhouette Score. This interactivity allows for a nuanced understanding of how different assumptions about 'similarity' affect the resulting financial market segments.

Use the **'Number of Clusters (k) for Hierarchical:'** slider, **'Linkage Method:'** selectbox, and **'Distance Metric:'** selectbox to experiment with different parameters.

<aside class="negative">
<b>Warning:</b> When the 'Linkage Method' is set to 'ward', the 'Distance Metric' must be 'euclidean'. The application will automatically adjust this for you if you select 'ward' with another metric.
</aside>

### Dendrogram Visualization
The application also provides a **Dendrogram Visualization**. The dendrogram is a tree-like diagram that visually represents the sequence of merges or splits.
-   Each leaf in the dendrogram represents a data point.
-   Branches merging together indicate that the clusters or data points are combined.
-   The height of the merge point on the y-axis indicates the distance at which the clusters were merged. Taller merges imply greater dissimilarity between the merged clusters.
-   A horizontal red dashed line will appear if you've selected more than one cluster, indicating the "cut-off" point that defines the specified number of clusters ($k$). Everything below this line will be grouped into one of the $k$ clusters.

Experiment with different linkage methods and distance metrics. You will observe how these choices significantly influence how clusters are formed and how sensitive the algorithm is to different data distributions. For instance, 'single' linkage can find elongated clusters, while 'ward' tends to find more compact, spherical clusters. The Silhouette Score helps quantify the effectiveness of these parameter choices. In finance, this allows tailoring the clustering approach to specific asset characteristics or market behaviors.

## 6. Conclusion and Further Exploration
Duration: 00:05
Congratulations! You have successfully explored three fundamental unsupervised clustering algorithms: k-Means, Spectral Clustering, and Agglomerative Hierarchical Clustering using the QuLab application.

### Key Takeaways:
-   **K-Means** is efficient and effective for finding globular clusters based on centroids. Its performance heavily relies on the initial choice of $k$.
-   **Spectral Clustering** excels at identifying complex, non-globular clusters by leveraging the underlying graph structure of the data, making it suitable for intricate relationships.
-   **Agglomerative Hierarchical Clustering** provides a hierarchical view of data relationships through a dendrogram, allowing for flexible cluster definition at different levels of granularity, and is highly influenced by the chosen linkage method and distance metric.

You've learned how to interactively tune parameters like the number of clusters ($k$), affinity kernels, linkage methods, and distance metrics, and how these choices directly impact the resulting cluster structures and their quality, as measured by the Silhouette Score.

We encourage you to:
-   **Experiment further:** Try different parameter combinations for each algorithm.
-   **Upload your own data:** Apply these powerful clustering techniques to your own financial datasets to uncover hidden patterns or segment your portfolio.
-   **Consider the context:** Always interpret clustering results in the context of your specific financial problem, as the "best" clustering is often domain-dependent.

Unsupervised learning is a cornerstone of modern data analysis in finance, offering invaluable insights for market segmentation, risk management, and portfolio optimization. Keep exploring!
