
# Streamlit Application Requirements Specification

## 1. Application Overview
This Streamlit application serves as an interactive workbench for exploring various unsupervised clustering techniques, specifically k-Means, Spectral, and Agglomerative Hierarchical Clustering. It aims to provide users with a clear understanding of these algorithms, their parameters, and their application in financial contexts through interactive visualizations and dynamic parameter tuning.

### Learning Goals
Upon completion of using this application, users will be able to:
*   Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms.
*   Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods, affinity kernels) on clustering results.
*   Visualize cluster assignments in 2D/3D space and interpret dendrograms generated by hierarchical clustering.
*   Gain insights into how clustering can be applied for tasks such as financial market segmentation or portfolio construction, with specific demonstrations using synthetic financial data.

## 2. User Interface Requirements

### Layout and Navigation Structure
The application will adopt a multi-page or tab-based structure to guide users through different clustering algorithms and functionalities.
*   **Sidebar (`st.sidebar`):**
    *   **Data Selection**: Radio buttons or a dropdown to select between pre-loaded datasets ('K-Means Portfolio Data', 'Spectral Assets Data') and a user-uploaded dataset.
    *   **File Uploader**: `st.file_uploader` for CSV files, visible when 'User Upload' is selected.
    *   **Global Parameters (if any)**: General settings applying across algorithms.
*   **Main Content Area**:
    *   **Tabs (`st.tabs` or `st.selectbox` for main sections):**
        *   **Introduction**: Overview and learning goals.
        *   **K-Means Clustering**: Theory, interactive tuning, and visualization.
        *   **Spectral Clustering**: Theory, interactive tuning, and visualization.
        *   **Agglomerative Hierarchical Clustering**: Theory, interactive tuning, and visualization, including dendrogram.
        *   **User Data Analysis**: Section to apply chosen algorithm to user data.

### Input Widgets and Controls
Interactive widgets will be primarily located in the sidebar for global data selection or within each algorithm-specific tab for parameter tuning.

*   **Data Input:**
    *   **Dataset Selector (`st.radio` or `st.selectbox`):** Options: "K-Means Portfolio Data (Demo)", "Spectral Assets Data (Demo)", "Upload Your Own Data".
    *   **File Uploader (`st.file_uploader`):** Accepts `.csv` files. Appears when "Upload Your Own Data" is selected.
*   **K-Means Parameters:**
    *   **Number of Clusters ($k$) Slider (`st.slider`):** Range from 2 to 10, default 3. Label: "Number of Clusters ($k$):".
*   **Spectral Clustering Parameters:**
    *   **Number of Clusters ($k$) Slider (`st.slider`):** Range from 2 to 10, default 3. Label: "Number of Clusters ($k$):".
    *   **Affinity Kernel Dropdown (`st.selectbox`):** Options: 'rbf', 'nearest\_neighbors'. Default 'rbf'. Label: "Affinity Kernel:".
*   **Agglomerative Hierarchical Clustering Parameters:**
    *   **Number of Clusters ($k$) Slider (`st.slider`):** Range from 2 to 10, default 3. Label: "Number of Clusters ($k$):".
    *   **Linkage Method Dropdown (`st.selectbox`):** Options: 'ward', 'complete', 'average', 'single'. Default 'ward'. Label: "Linkage Method:".
    *   **Distance Metric Dropdown (`st.selectbox`):** Options: 'euclidean', 'l1', 'l2', 'manhattan', 'cosine'. Default 'euclidean'. Label: "Distance Metric:".
        *   *Note*: 'ward' linkage will restrict 'euclidean' metric selection, and other metrics will be disabled or an error message will be shown if 'ward' is selected.

### Visualization Components (Charts, Graphs, Tables)
All visualizations will be interactive, primarily using Plotly for scatter plots and Matplotlib for dendrograms.

*   **2D Scatter Plots (`st.plotly_chart`):**
    *   Display data points colored by assigned cluster.
    *   K-Means plots will include cluster centroids as distinct markers ('x').
    *   Axis labels: 'Feature 1', 'Feature 2' (or dynamic labels for user data).
    *   Title: Reflecting algorithm, $k$, and other key parameters.
    *   Hover data: Show cluster assignment.
    *   Legends: Clearly differentiate clusters and centroids.
*   **Interactive Dendrogram (`st.pyplot`):**
    *   For Agglomerative Hierarchical Clustering.
    *   Show the hierarchical merging process.
    *   Optionally display a cut-off line corresponding to the selected number of clusters ($k$).
    *   Labels: "Sample Index or (Cluster Size)" for X-axis, "Distance" for Y-axis.
    *   Title: "Hierarchical Clustering Dendrogram with [Linkage Method] Linkage".
*   **Metrics Display (`st.metric` or `st.info`):**
    *   Silhouette Score: Displayed numerically (e.g., "Silhouette Score: 0.XXX") for k-Means, Spectral, and Agglomerative Clustering.

### Interactive Elements and Feedback Mechanisms
*   **Dynamic Updates**: All plots and metrics will automatically update in real-time as users adjust parameters via sliders and dropdowns.
*   **Informative Messages**: `st.write`, `st.info`, `st.warning`, `st.error` will be used for:
    *   Data loading status and shape.
    *   Silhouette Score values and calculation caveats.
    *   Validation messages for input parameters (e.g., $k$ range, 'ward' linkage restrictions).
    *   Feedback on successful data upload and preprocessing.

## 3. Additional Requirements

### Annotation and Tooltip Specifications
*   **Markdown Explanations**: Extensive use of `st.markdown` to include theoretical explanations, algorithm steps, and financial interpretations as provided in the Jupyter Notebook. This includes all LaTeX-formatted mathematical equations.
*   **Widget Descriptions**: Concise labels and potentially `st.help` or brief `st.info` messages for sliders and dropdowns to clarify their purpose.
*   **Plot Titles & Legends**: Clear and descriptive titles for all plots and comprehensive legends for cluster identification.

### Save the States of the Fields Properly so that Changes are Not Lost
*   **Session State (`st.session_state`):**
    *   The loaded data (both synthetic and user-uploaded) will be stored in `st.session_state` to persist across user interactions and avoid re-loading/re-processing on every rerun.
    *   Selected algorithm parameters (e.g., `n_clusters`, `linkage_method`, `affinity_kernel`) will be managed using `st.session_state` to maintain user choices as they navigate between sections or make adjustments.

## 4. Notebook Content and Code Requirements

### Global Imports and Initial Setup
The application will begin with standard library imports, `st.set_page_config`, and initial data loading.

```python
import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Scikit-learn imports
from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs, make_circles
from sklearn.metrics import silhouette_score
# PCA not explicitly used for clustering in notebook, but included in original imports.
# from sklearn.decomposition import PCA

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(layout="wide", page_title="Interactive Clustering Workbench")
st.title("Interactive Clustering Workbench")

st.markdown("""
This Streamlit application provides an interactive platform for exploring and understanding various unsupervised clustering techniques. Unsupervised learning is a powerful paradigm in machine learning that aims to discover hidden patterns or intrinsic structures in unlabeled data. In financial markets, this can be invaluable for tasks such as market segmentation, identifying correlated assets, or constructing diversified portfolios.
""")

st.markdown("### Learning Goals")
st.markdown("""
Upon completion of this application, users will be able to:
- Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms.
- Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods, affinity kernels) on clustering results.
- Visualize cluster assignments in 2D/3D space and interpret dendrograms generated by hierarchical clustering.
- Gain insights into how clustering can be applied for tasks such as financial market segmentation or portfolio construction, with specific demonstrations using synthetic financial data.
""")
```

### 2. Setting Up the Environment and Data Utilities
This section defines the utility functions for data generation and preprocessing, essential for both synthetic and user-uploaded data.

```python
st.markdown("## 2. Setting Up the Environment and Data Utilities")
st.markdown("""
We begin by defining utility functions for generating our synthetic financial datasets and for preprocessing them. These functions ensure that our data is in a suitable format for clustering algorithms and allow for easy experimentation with different datasets.
""")

def load_synthetic_financial_data(dataset_type):
    """
    Generates and returns a synthetic financial dataset based on the specified type.
    
    Arguments:
        dataset_type (str): A string indicating the type of synthetic dataset to generate
                            (e.g., 'kmeans_portfolio', 'spectral_assets').
    
    Output:
        X (np.ndarray): The generated synthetic dataset.
    """
    if not isinstance(dataset_type, str):
        raise TypeError("dataset_type must be a string.")

    if dataset_type == 'kmeans_portfolio':
        X, _ = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=42)
        return X
    elif dataset_type == 'spectral_assets':
        X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)
        return X
    elif dataset_type == "":
        raise ValueError("dataset_type cannot be an empty string.")
    else:
        raise ValueError(f"Unsupported dataset_type: '{dataset_type}'.")

def preprocess_data(data: (np.ndarray | pd.DataFrame)) -> np.ndarray:
    """
    Applies StandardScaler from sklearn.preprocessing to the input data, ensuring all features are
    scaled to a standard range. This preprocessing step is crucial for distance-based clustering
    algorithms to prevent features with larger ranges from dominating the distance calculations.

    Arguments:
        data (np.ndarray or pd.DataFrame): The input data to be preprocessed.

    Output:
        scaled_data (np.ndarray): The scaled data with standardized features.
    """
    if not isinstance(data, (np.ndarray, pd.DataFrame)):
        raise TypeError("Input data must be a numpy.ndarray or a pandas.DataFrame.")

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    return scaled_data

st.markdown("""
The `load_synthetic_financial_data` function generates specific synthetic datasets that mimic financial market data characteristics for demonstration purposes. The `preprocess_data` function ensures that all features are scaled to a standard range, which is crucial for distance-based clustering algorithms as it prevents features with larger numerical ranges from disproportionately influencing the distance calculations.
""")

# Initialize session state for data
if 'scaled_kmeans_data' not in st.session_state:
    st.session_state.kmeans_data_raw = load_synthetic_financial_data('kmeans_portfolio')
    st.session_state.scaled_kmeans_data = preprocess_data(st.session_state.kmeans_data_raw)

if 'scaled_spectral_data' not in st.session_state:
    st.session_state.spectral_data_raw = load_synthetic_financial_data('spectral_assets')
    st.session_state.scaled_spectral_data = preprocess_data(st.session_state.spectral_data_raw)

if 'user_data_df' not in st.session_state:
    st.session_state.user_data_df = None
    st.session_state.scaled_user_data = None
```

### Data Selection and Upload (Sidebar Component)
The sidebar will host the data selection mechanism.

```python
st.sidebar.header("Data Source Selection")
data_source_option = st.sidebar.radio(
    "Choose Data Source:",
    ("K-Means Portfolio Data (Demo)", "Spectral Assets Data (Demo)", "Upload Your Own Data")
)

current_data = None
data_name = ""

if data_source_option == "K-Means Portfolio Data (Demo)":
    current_data = st.session_state.scaled_kmeans_data
    data_name = "K-Means Portfolio Data"
    st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
elif data_source_option == "Spectral Assets Data (Demo)":
    current_data = st.session_state.scaled_spectral_data
    data_name = "Spectral Assets Data"
    st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
elif data_source_option == "Upload Your Own Data":
    uploaded_file = st.sidebar.file_uploader("Upload CSV file", type=["csv"])
    if uploaded_file is not None:
        try:
            st.session_state.user_data_df = pd.read_csv(uploaded_file)
            st.session_state.scaled_user_data = preprocess_data(st.session_state.user_data_df.values)
            current_data = st.session_state.scaled_user_data
            data_name = "Uploaded User Data"
            st.sidebar.success("CSV file successfully uploaded and preprocessed!")
            st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
        except Exception as e:
            st.sidebar.error(f"Error processing uploaded file: {e}")
            current_data = None
    else:
        st.session_state.user_data_df = None
        st.session_state.scaled_user_data = None
        current_data = None # Ensure no data is selected if no file is uploaded
        st.sidebar.warning("Please upload a CSV file for analysis.")

# Fallback if current_data is None (e.g., initial state of user upload)
if current_data is None:
    st.warning("Please select a data source or upload a valid CSV to proceed with clustering.")
    # Use a dummy data to prevent errors in clustering sections if no data is available
    # Or disable clustering sections until data is valid.
    # For now, let's keep it None and handle in sections.
```

### Main Application Tabs
The core functionality will be organized into tabs.

```python
kmeans_tab, spectral_tab, hierarchical_tab = st.tabs([
    "K-Means Clustering", "Spectral Clustering", "Agglomerative Hierarchical Clustering"
])
```

### K-Means Clustering Section (`kmeans_tab`)

```python
with kmeans_tab:
    st.markdown("## 4. K-Means Clustering: Theory")
    st.markdown("""
    k-Means clustering is a partition-based algorithm that aims to partition $n$ observations into $k$ clusters, where each observation belongs to the cluster with the nearest mean (centroid). The algorithm iteratively assigns data points to clusters and updates cluster centroids until convergence.

    The objective function, often called the inertia, that k-Means aims to minimize is the sum of squared distances of samples to their closest cluster center:
    $$ J = \\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2) $$
    where $J$ is the objective function, $x_i$ is a data point, $\\mu_j$ is the centroid of cluster $j$, and $C$ is the set of all centroids. The algorithm typically proceeds as follows:
    1. Initialize $k$ centroids randomly.
    2. For a fixed number of iterations:
       a. Assign each data point to its closest centroid.
       b. Update each centroid to be the mean of all points assigned to that cluster.
       c. If centroids do not change significantly, break.
    3. Output cluster assignments and final centroids.
    """)

    def run_kmeans(data, n_clusters, random_state=42):
        """Runs K-Means clustering."""
        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')
        kmeans.fit(data)
        return kmeans.labels_, kmeans.cluster_centers_

    st.markdown("""
    The `run_kmeans` function encapsulates the k-Means algorithm, allowing us to easily apply it with different parameters. This abstraction simplifies the process of exploring various cluster configurations and their impact on data segmentation.
    """)

    st.markdown("## 5. K-Means Clustering: Interactive Parameter Tuning and Evaluation")
    st.markdown("""
    The choice of the number of clusters, $k$, is critical for k-Means. We can evaluate the quality of clustering using metrics like the Silhouette Score. A higher Silhouette Score generally indicates better-defined clusters, where data points are well-matched to their own cluster and poorly matched to neighboring clusters. The Silhouette Coefficient $s(i)$ for a single sample is calculated as:
    $$ s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$
    where $a(i)$ is the mean distance between $i$ and all other data points in the same cluster, and $b(i)$ is the mean distance between $i$ and all other data points in the *next nearest* cluster.

    We can interactively adjust $k$ and observe its impact on the clusters and the Silhouette Score. This helps in finding an optimal $k$ that balances clustering quality with business interpretability.
    """)

    if current_data is not None:
        if current_data.shape[1] < 2:
            st.warning("K-Means clustering requires at least 2 features for 2D visualization. Please upload data with more features or select a demo dataset.")
        else:
            n_clusters_kmeans = st.slider('Number of Clusters (k) for K-Means:', min_value=2, max_value=10, value=3, key='kmeans_n_clusters')

            kmeans_labels, kmeans_centers = run_kmeans(current_data, n_clusters=n_clusters_kmeans)

            if n_clusters_kmeans > 1:
                silhouette_avg_kmeans = silhouette_score(current_data, kmeans_labels)
                st.metric(label=f"Silhouette Score for {n_clusters_kmeans} clusters:", value=f"{silhouette_avg_kmeans:.3f}")
            else:
                st.info("Silhouette Score cannot be calculated for less than 2 clusters.")

            kmeans_df_interactive = pd.DataFrame(current_data[:, :2], columns=['Feature 1', 'Feature 2']) # Limit to 2 features for 2D plot
            kmeans_df_interactive['Cluster'] = kmeans_labels.astype(str)

            fig_kmeans = px.scatter(kmeans_df_interactive, x='Feature 1', y='Feature 2', color='Cluster',
                                    title=f'K-Means Clustering with k={n_clusters_kmeans} of {data_name}',
                                    hover_data=['Cluster'])

            centroids_df_interactive = pd.DataFrame(kmeans_centers[:, :2], columns=['Feature 1', 'Feature 2'])
            centroids_df_interactive['Cluster'] = [f'Centroid {i}' for i in range(len(kmeans_centers))]

            fig_kmeans.add_scatter(x=centroids_df_interactive['Feature 1'], y=centroids_df_interactive['Feature 2'],
                                mode='markers', marker=dict(symbol='x', size=15, color='black', line=dict(width=2)),
                                name='Centroids', showlegend=True)

            fig_kmeans.update_layout(legend_title_text='Cluster / Centroid')
            st.plotly_chart(fig_kmeans, use_container_width=True)

            st.markdown("""
            The scatter plot visually represents the distinct clusters identified by the k-Means algorithm. Each color corresponds to a different cluster, and the black 'x' markers indicate the calculated centroids. In a financial context, these clusters could represent different types of stocks (e.g., growth stocks, value stocks, defensive stocks), aiding in diversified portfolio construction by selecting assets from different segments.
            By adjusting the slider, you can observe how the cluster boundaries and assignments change. The Silhouette Score provides a quantitative measure of clustering quality, helping to identify a suitable number of clusters for the given dataset.
            """)
    else:
        st.info("Please select a data source from the sidebar to run K-Means clustering.")
```

### Spectral Clustering Section (`spectral_tab`)

```python
with spectral_tab:
    st.markdown("## 8. Spectral Clustering: Theory")
    st.markdown("""
    Spectral Clustering is a technique that uses the eigenvalues (spectrum) of a similarity matrix to perform dimensionality reduction before clustering in a lower-dimensional space. It is particularly effective for discovering non-globular or intertwined clusters by leveraging the graph structure of the data. The process generally follows these steps:
    1.  **Construct similarity matrix $W$**: Measures the similarity between all pairs of data points. A common choice is the Gaussian (Radial Basis Function) kernel:
        $$ W[i, j] = \\exp\\left(-\\frac{||x_i - x_j||^2}{2\\sigma^2}\\right) $$
        where $x_i$ and $x_j$ are data points, and $\\sigma$ is a scaling parameter that controls the width of the similarity decay.
    2.  **Compute degree matrix $D$**: A diagonal matrix where $D[i, i]$ is the sum of similarities of data point $i$ with all other data points:
        $$ D[i, i] = \\sum_j W[i, j] $$
    3.  **Compute normalized Laplacian $L_{norm}$**: A matrix often used to reveal the graph structure and its connected components:
        $$ L_{norm} = D^{-1/2} \\times (D - W) \\times D^{-1/2} $$
    4.  **Find $k$ smallest eigenvectors of $L_{norm}$**: These eigenvectors correspond to the lowest frequencies in the graph and form a new lower-dimensional, spectrally transformed representation of the data.
    5.  **Form matrix $V$**: Consisting of these $k$ eigenvectors as columns.
    6.  **Normalize rows of $V$ to unit length**.
    7.  **Apply k-Means clustering to the rows of $V$**: This groups the data points in the transformed, lower-dimensional space.
    8.  **Output cluster assignments**.
    """)

    def run_spectral_clustering(data, n_clusters, affinity_kernel, gamma, random_state=42):
        """
        Executes Spectral Clustering on the given data using sklearn.cluster.SpectralClustering.
        """
        model = SpectralClustering(
            n_clusters=n_clusters,
            affinity=affinity_kernel,
            gamma=gamma,
            random_state=random_state,
            n_init=10
        )
        cluster_labels = model.fit_predict(data)
        return cluster_labels

    st.markdown("""
    The `run_spectral_clustering` function simplifies the application of Spectral Clustering with flexible parameters like the number of clusters and the affinity kernel. This allows us to efficiently explore how different similarity measures impact the discovery of intricate data structures.
    """)

    st.markdown("## 10. Spectral Clustering: Interactive Parameter Tuning and Evaluation")
    st.markdown("""
    Similar to k-Means, the number of clusters $k$ is an important parameter for Spectral Clustering. Additionally, the `affinity` kernel (how similarity is defined) significantly impacts the clustering results. We can explore these parameters interactively, along with the Silhouette Score, to understand their influence on cluster formation and quality.
    """)

    if current_data is not None:
        if current_data.shape[1] < 2:
            st.warning("Spectral clustering requires at least 2 features for 2D visualization. Please upload data with more features or select a demo dataset.")
        else:
            col1, col2 = st.columns(2)
            n_clusters_spectral = col1.slider('Number of Clusters (k) for Spectral Clustering:', min_value=2, max_value=10, value=3, key='spectral_n_clusters')
            affinity_kernel_spectral = col2.selectbox('Affinity Kernel for Spectral Clustering:', options=['rbf', 'nearest_neighbors'], value='rbf', key='spectral_affinity_kernel')
            gamma_val_spectral = 1.0 # Default gamma for rbf, not made interactive for brevity

            try:
                spectral_labels = run_spectral_clustering(current_data, n_clusters_spectral, affinity_kernel_spectral, gamma_val_spectral)

                unique_labels_count_spectral = len(np.unique(spectral_labels))
                if unique_labels_count_spectral > 1 and unique_labels_count_spectral <= len(current_data) -1:
                    silhouette_avg_spectral = silhouette_score(current_data, spectral_labels)
                    st.metric(label=f"Silhouette Score for {n_clusters_spectral} clusters:", value=f"{silhouette_avg_spectral:.3f}")
                else:
                    st.info(f"Silhouette Score cannot be computed for {unique_labels_count_spectral} unique clusters (requires >1 and < n_samples).")

                spectral_df_interactive = pd.DataFrame(current_data[:, :2], columns=['Feature 1', 'Feature 2'])
                spectral_df_interactive['Cluster'] = spectral_labels.astype(str)

                title_score_spectral = f'Silhouette Score: {silhouette_avg_spectral:.3f}' if unique_labels_count_spectral > 1 else 'Silhouette Score: N/A'
                fig_spectral = px.scatter(spectral_df_interactive, x='Feature 1', y='Feature 2', color='Cluster',
                                        title=f'Spectral Clustering (k={n_clusters_spectral}, Kernel="{affinity_kernel_spectral}") of {data_name}<br>{title_score_spectral}',
                                        hover_data=['Cluster'])
                fig_spectral.update_layout(legend_title_text='Cluster')
                st.plotly_chart(fig_spectral, use_container_width=True)

                st.markdown("""
                Observe how changes in `n_clusters` and the `affinity_kernel` affect the resulting clusters and the Silhouette Score. Different affinity kernels might be more appropriate for different underlying data structures. For example, 'rbf' is good for capturing dense regions and non-linear boundaries, while 'nearest_neighbors' focuses on local connectivity. This interactive exploration helps in selecting parameters that yield meaningful and stable clusters for financial assets.
                """)
            except Exception as e:
                st.error(f"Spectral Clustering failed with given parameters: {e}")
    else:
        st.info("Please select a data source from the sidebar to run Spectral Clustering.")
```

### Agglomerative Hierarchical Clustering Section (`hierarchical_tab`)

```python
with hierarchical_tab:
    st.markdown("## 11. Agglomerative Hierarchical Clustering: Theory")
    st.markdown("""
    Agglomerative Hierarchical Clustering builds a hierarchy of clusters from individual data points. It is a "bottom-up" approach, where each data point starts as its own cluster, and then pairs of clusters are iteratively merged based on their proximity until all points belong to a single cluster or a desired number of clusters is reached. This process forms a tree-like structure called a dendrogram.

    Key concepts include:
    -   **Distance Metric**: How the distance between individual data points is measured (e.g., Euclidean, Manhattan, Cosine). This defines the proximity between any two data points.
    -   **Linkage Method**: How the distance between two clusters is defined based on the distances between their constituent data points. Common methods include:
        -   **Single Linkage**: The shortest distance between any two points in the two clusters. This method is prone to chaining, where clusters are merged due to single close points.
            $$ d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y) $$
        -   **Complete Linkage**: The maximum distance between any two points in the two clusters. This tends to produce compact, spherical clusters.
            $$ d(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x, y) $$
        -   **Average Linkage**: The average distance between all pairs of points across the two clusters. This offers a compromise between single and complete linkage.
            $$ d(C_i, C_j) = \\text{mean}_{x \\in C_i, y \\in C_j} d(x, y) $$
        -   **Ward Linkage**: Minimizes the variance of the clusters being merged. It calculates the increase in the total within-cluster variance after merging, generally favoring compact, spherical clusters of similar size.

    Understanding these methods is crucial for financial applications, as the choice impacts how asset groups are formed (e.g., tightly correlated groups vs. loosely related ones).
    """)

    def run_hierarchical_clustering(data, n_clusters, linkage_method, affinity_metric):
        """Runs Agglomerative Hierarchical Clustering."""
        agg_clustering = AgglomerativeClustering(
            n_clusters=n_clusters,
            linkage=linkage_method,
            metric=affinity_metric
        )
        cluster_labels = agg_clustering.fit_predict(data)
        return cluster_labels

    def plot_dendrogram(data, linkage_matrix, n_clusters_display=None, title="Hierarchical Clustering Dendrogram"):
        """
        Generates and displays an interactive dendrogram using Matplotlib.
        """
        if not isinstance(data, np.ndarray):
            raise TypeError("Input 'data' must be a numpy array.")
        
        N = data.shape[0]

        fig, ax = plt.subplots(figsize=(12, 8))
        ax.set_title(title)

        color_threshold_param = None
        cut_off_height = None
        add_axhline = False

        if n_clusters_display is not None:
            if not isinstance(n_clusters_display, int):
                raise TypeError("n_clusters_display must be an integer or None.")

            if n_clusters_display == 0:
                color_threshold_param = 0
                add_axhline = False
            elif n_clusters_display > 0:
                if linkage_matrix.shape[0] > 0:
                    if n_clusters_display <= N:
                        index_for_threshold = -(n_clusters_display - 1)
                        if index_for_threshold < linkage_matrix.shape[0]: # Ensure index is valid
                            color_threshold_param = linkage_matrix[index_for_threshold, 2]
                            cut_off_height = color_threshold_param
                            add_axhline = (n_clusters_display > 1)
                        else: # Not enough merges to form that many clusters
                            color_threshold_param = None
                            cut_off_height = None
                            add_axhline = False
                    else: # n_clusters > N, not possible
                        color_threshold_param = None
                        cut_off_height = None
                        add_axhline = False
                else: # linkage_matrix is empty, typically for N=1
                    if n_clusters_display == 1 and N == 1:
                        color_threshold_param = 0
                        cut_off_height = 0
                        add_axhline = False
                    else:
                        color_threshold_param = None
                        cut_off_height = None
                        add_axhline = False

        dendrogram(
            linkage_matrix, 
            color_threshold=color_threshold_param, 
            above_threshold_color='k',
            ax=ax
        )

        if add_axhline and cut_off_height is not None:
            ax.axhline(y=cut_off_height, color='r', linestyle='--', label=f'{n_clusters_display} Clusters Cut-off')
            ax.legend()

        ax.set_xlabel("Sample Index or (Cluster Size)")
        ax.set_ylabel("Distance")
        ax.grid(True, linestyle='--', alpha=0.6)
        st.pyplot(fig)


    st.markdown("""
    The `run_hierarchical_clustering` function allows us to experiment with different linkage methods and distance metrics, providing flexibility in how clusters are formed. The `plot_dendrogram` function provides a visual representation of the hierarchical structure, which is crucial for understanding the merging process and for selecting an appropriate number of clusters by observing the 'cuts' in the tree.
    """)

    st.markdown("## 13. Agglomerative Hierarchical Clustering: Interactive Parameter Tuning and Evaluation")
    st.markdown("""
    The power of hierarchical clustering lies in exploring different linkage methods and distance metrics. We can interactively adjust these parameters along with the desired number of clusters ($k$) to understand their impact on the clustering structure and the Silhouette Score. This interactivity allows for a nuanced understanding of how different assumptions about 'similarity' affect the resulting financial market segments.
    """)

    if current_data is not None:
        if current_data.shape[1] < 2:
            st.warning("Agglomerative Hierarchical clustering requires at least 2 features for 2D visualization. Please upload data with more features or select a demo dataset.")
        else:
            col1, col2, col3 = st.columns(3)
            n_clusters_agg = col1.slider('Number of Clusters (k) for Hierarchical:', min_value=2, max_value=10, value=3, key='agg_n_clusters')
            linkage_method_agg = col2.selectbox('Linkage Method:', options=['ward', 'complete', 'average', 'single'], value='ward', key='agg_linkage_method')
            affinity_metric_agg = col3.selectbox('Distance Metric:', options=['euclidean', 'l1', 'l2', 'manhattan', 'cosine'], value='euclidean', key='agg_affinity_metric')

            # Specific validation for 'ward' linkage
            if linkage_method_agg == 'ward' and affinity_metric_agg != 'euclidean':
                st.warning("When linkage_method is 'ward', affinity_metric must be 'euclidean'. Changing metric to 'euclidean'.")
                affinity_metric_agg = 'euclidean' # Force correction for ward

            try:
                agg_labels = run_hierarchical_clustering(current_data, n_clusters=n_clusters_agg, linkage_method=linkage_method_agg, affinity_metric=affinity_metric_agg)

                unique_labels_agg = np.unique(agg_labels)
                if len(unique_labels_agg) < 2 or len(unique_labels_agg) > len(current_data) -1:
                    silhouette_avg_agg = -1.0
                    st.info(f"Silhouette Score cannot be computed as {len(unique_labels_agg)} unique clusters were formed (requires >1 and < n_samples).")
                else:
                    silhouette_avg_agg = silhouette_score(current_data, agg_labels)
                    st.metric(label=f"Silhouette Score for {n_clusters_agg} clusters:", value=f"{silhouette_avg_agg:.4f}")

                agg_df_interactive = pd.DataFrame(current_data[:, :2], columns=['Feature 1', 'Feature 2'])
                agg_df_interactive['Cluster'] = agg_labels.astype(str)

                title_score_agg = f'Silhouette Score: {silhouette_avg_agg:.4f}' if silhouette_avg_agg != -1.0 else 'Silhouette Score: N/A'
                fig_agg = px.scatter(agg_df_interactive, x='Feature 1', y='Feature 2', color='Cluster',
                                    title=f'Agglomerative Hierarchical Clustering (k={n_clusters_agg}, Linkage: {linkage_method_agg}, Metric: {affinity_metric_agg}) of {data_name}<br>{title_score_agg}',
                                    hover_data=['Cluster'])
                fig_agg.update_layout(legend_title_text='Cluster')
                st.plotly_chart(fig_agg, use_container_width=True)

                st.markdown("---")
                st.markdown("### Dendrogram Visualization")
                # Compute linkage matrix for dendrogram
                Z = linkage(current_data, method=linkage_method_agg, metric=affinity_metric_agg)
                plot_dendrogram(current_data, Z, n_clusters_display=n_clusters_agg, 
                                title=f"Hierarchical Clustering Dendrogram with {linkage_method_agg.title()} Linkage")

                st.markdown("""
                Experiment with different linkage methods and distance metrics. You will observe how these choices significantly influence how clusters are formed and how sensitive the algorithm is to different data distributions. For instance, 'single' linkage can find elongated clusters, while 'ward' tends to find more compact, spherical clusters. The Silhouette Score helps quantify the effectiveness of these parameter choices. In finance, this allows tailoring the clustering approach to specific asset characteristics or market behaviors.
                """)
            except Exception as e:
                st.error(f"Agglomerative Hierarchical Clustering failed with given parameters: {e}")
    else:
        st.info("Please select a data source from the sidebar to run Agglomerative Hierarchical Clustering.")
```

