"""app.py"""
import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

# Scikit-learn imports
from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs, make_circles
from sklearn.metrics import silhouette_score

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(page_title="QuLab", layout="wide")
st.sidebar.image("https://www.quantuniversity.com/assets/img/logo5.jpg")
st.sidebar.divider()
st.title("QuLab")
st.divider()
st.markdown("""
In this lab, we explore and understand various unsupervised clustering techniques, specifically k-Means, Spectral, and Agglomerative Hierarchical Clustering.
Unsupervised learning is a powerful paradigm in machine learning that aims to discover hidden patterns or intrinsic structures in unlabeled data. In financial markets, this can be invaluable for tasks such as market segmentation, identifying correlated assets, or constructing diversified portfolios.

### Learning Goals
Upon completion of this application, users will be able to:
- Understand the operational principles of k-Means, Spectral, and Agglomerative Hierarchical clustering algorithms.
- Analyze the impact of different parameters (e.g., number of clusters $k$, linkage methods, affinity kernels) on clustering results.
- Visualize cluster assignments in 2D/3D space and interpret dendrograms generated by hierarchical clustering.
- Gain insights into how clustering can be applied for tasks such as financial market segmentation or portfolio construction, with specific demonstrations using synthetic financial data.
""")

# 2. Setting Up the Environment and Data Utilities
st.markdown("## 2. Setting Up the Environment and Data Utilities")
st.markdown("""
We begin by defining utility functions for generating our synthetic financial datasets and for preprocessing them. These functions ensure that our data is in a suitable format for clustering algorithms and allow for easy experimentation with different datasets.
""")

def load_synthetic_financial_data(dataset_type):
    """Docstring: Generates and returns a synthetic financial dataset based on the specified type.
    
    Arguments:
        dataset_type (str): A string indicating the type of synthetic dataset to generate
                            (e.g., 'kmeans_portfolio', 'spectral_assets').
    
    Output:
        X (np.ndarray): The generated synthetic dataset.
    """
    if not isinstance(dataset_type, str):
        raise TypeError("dataset_type must be a string.")

    if dataset_type == 'kmeans_portfolio':
        X, _ = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=42)
        return X
    elif dataset_type == 'spectral_assets':
        X, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)
        return X
    elif dataset_type == "":
        raise ValueError("dataset_type cannot be an empty string.")
    else:
        raise ValueError(f"Unsupported dataset_type: '{dataset_type}'.")

def preprocess_data(data):
    """Docstring: Applies StandardScaler from sklearn.preprocessing to the input data, ensuring all features are
    scaled to a standard range. This preprocessing step is crucial for distance-based clustering
    algorithms to prevent features with larger ranges from dominating the distance calculations.

    Arguments:
        data (np.ndarray or pd.DataFrame): The input data to be preprocessed.

    Output:
        scaled_data (np.ndarray): The scaled data with standardized features.
    """
    if not isinstance(data, (np.ndarray, pd.DataFrame)):
        raise TypeError("Input data must be a numpy.ndarray or a pandas.DataFrame.")

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)
    
    return scaled_data

st.markdown("""
The `load_synthetic_financial_data` function generates specific synthetic datasets that mimic financial market data characteristics for demonstration purposes. The `preprocess_data` function ensures that all features are scaled to a standard range, which is crucial for distance-based clustering algorithms as it prevents features with larger numerical ranges from disproportionately influencing the distance calculations.
""")

# Initialize session state for data
if 'kmeans_data_raw' not in st.session_state:
    st.session_state.kmeans_data_raw = load_synthetic_financial_data('kmeans_portfolio')
    st.session_state.scaled_kmeans_data = preprocess_data(st.session_state.kmeans_data_raw)

if 'spectral_data_raw' not in st.session_state:
    st.session_state.spectral_data_raw = load_synthetic_financial_data('spectral_assets')
    st.session_state.scaled_spectral_data = preprocess_data(st.session_state.spectral_data_raw)

if 'user_data_df' not in st.session_state:
    st.session_state.user_data_df = None
    st.session_state.scaled_user_data = None

# Data Selection and Upload (Sidebar Component)
st.sidebar.header("Data Source Selection")
data_source_option = st.sidebar.radio(
    "Choose Data Source:",
    ("K-Means Portfolio Data (Demo)", "Spectral Assets Data (Demo)", "Upload Your Own Data")
)

current_data = None
data_name = ""

if data_source_option == "K-Means Portfolio Data (Demo)":
    current_data = st.session_state.scaled_kmeans_data
    data_name = "K-Means Portfolio Data"
    st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
elif data_source_option == "Spectral Assets Data (Demo)":
    current_data = st.session_state.scaled_spectral_data
    data_name = "Spectral Assets Data"
    st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
elif data_source_option == "Upload Your Own Data":
    uploaded_file = st.sidebar.file_uploader("Upload CSV file", type=["csv"])
    if uploaded_file is not None:
        try:
            st.session_state.user_data_df = pd.read_csv(uploaded_file)
            st.session_state.scaled_user_data = preprocess_data(st.session_state.user_data_df.values)
            current_data = st.session_state.scaled_user_data
            data_name = "Uploaded User Data"
            st.sidebar.success("CSV file successfully uploaded and preprocessed!")
            st.sidebar.info(f"Loaded {data_name} with shape: {current_data.shape}")
        except Exception as e:
            st.sidebar.error(f"Error processing uploaded file: {e}")
            current_data = None
    else:
        st.session_state.user_data_df = None
        st.session_state.scaled_user_data = None
        current_data = None # Ensure no data is selected if no file is uploaded
        st.sidebar.warning("Please upload a CSV file for analysis.")

st.session_state.current_data = current_data
st.session_state.data_name = data_name

# Fallback if current_data is None (e.g., initial state of user upload)
if current_data is None:
    st.warning("Please select a data source or upload a valid CSV to proceed with clustering.")

# Your code starts here
page = st.sidebar.selectbox(label="Navigation", options=["K-Means Clustering", "Spectral Clustering", "Agglomerative Hierarchical Clustering"])
if page == "K-Means Clustering":
    from application_pages.page1 import run_page1
    run_page1()
elif page == "Spectral Clustering":
    from application_pages.page2 import run_page2
    run_page2()
elif page == "Agglomerative Hierarchical Clustering":
    from application_pages.page3 import run_page3
    run_page3()
# Your code ends
